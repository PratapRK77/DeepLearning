{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e30d2d6-d362-4ca7-9767-35d85c8b6d93",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34914914-e72a-42d8-807a-2d11d04d0302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "491cf0f1-c6db-4b99-aa58-714d19535a40",
   "metadata": {},
   "source": [
    "### Introduction to Deep Learning Assignment questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a2038-e289-48f4-856b-174d0d3f2f23",
   "metadata": {},
   "source": [
    "## 1.Explain what deep learning is and discuss its significance in the broader field of artificial intelligence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3fdabc-69e7-4fbc-a488-f3d6beaccbed",
   "metadata": {},
   "source": [
    "### Deep learning :\n",
    "is a subset of machine learning that uses artificial neural networks with multiple layers to learn from large amounts of data and identify complex patterns. It mimics the human brain's architecture to enable tasks such as image recognition, speech processing, and natural language understanding.\n",
    "\n",
    "### Significance in AI:\n",
    "\n",
    "High accuracy: Deep learning has surpassed traditional machine learning in tasks like image classification and language translation.\n",
    "\n",
    "Automated feature extraction: It learns relevant features directly from raw data, reducing the need for manual feature engineering.\n",
    "\n",
    "Real-world applications: Powers systems like self-driving cars, voice assistants, recommendation engines, and medical diagnostics.\n",
    "\n",
    "Adaptability: Supports transfer learning, making it versatile for different tasks with minimal additional data.\n",
    "\n",
    "Overall, deep learning has been transformative in pushing the boundaries of what AI can achieve, enabling advanced technology that improves efficiency and enhances user experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062847e-ebf1-4f89-a4b6-7d226173d79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7a77514-ebe7-484e-9294-044fdb74217d",
   "metadata": {},
   "source": [
    "## 2. List and explain the fundamental components of artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf45e86-f3c8-4be9-8bfc-69f360784ed7",
   "metadata": {},
   "source": [
    "### Fundamental Components of Artificial Neural Networks (ANNs):\n",
    "\n",
    "1.Neurons (Nodes): Basic units that receive input, process it, and pass output to the next layer.\n",
    "\n",
    "2.Input Layer: The first layer that receives the input data.\n",
    "    \n",
    "3.Hidden Layers: Intermediate layers that transform inputs into complex patterns using weights, biases, and activation functions.\n",
    "    \n",
    "4.Output Layer: The final layer that produces the model's output.\n",
    "\n",
    "5.Weights: Parameters that control the strength of connections between neurons and are adjusted during training.\n",
    "    \n",
    "6.Bias: An additional parameter that shifts the activation function, helping the model fit data better.\n",
    "\n",
    "7.Activation Function: A function (e.g., ReLU, sigmoid) applied to the weighted sum to introduce non-linearity.\n",
    "\n",
    "8.Loss Function: Measures how far the network’s predictions are from actual values; used for model evaluation.\n",
    "    \n",
    "9.Optimizer: Adjusts weights and biases to minimize the loss function, using algorithms like gradient descent.\n",
    "\n",
    "10.Forward Propagation: The process of passing input data through the network to generate an output.\n",
    "    \n",
    "11.Backpropagation: The process of updating weights and biases by calculating gradients to minimize the loss function.\n",
    "    \n",
    "These components enable ANNs to learn from data, recognize patterns, and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe7a51-57d5-4c30-bb31-d72217b97d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d63841-c77c-469f-b3d0-5d04dc58867f",
   "metadata": {},
   "source": [
    "##  3.Discuss the roles of neurons, connections, weights, and biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5279771-936d-4af4-88bc-83a15ee45078",
   "metadata": {},
   "source": [
    "### Roles of Neurons, Connections, Weights, and Biases in Neural Networks:\n",
    "\n",
    "### 1.Neurons (Nodes):\n",
    "\n",
    "Role: Neurons are the fundamental units of a neural network that process and transmit information. Each neuron receives input, applies a weighted sum, adds a bias, and passes the result through an activation function to produce an output.\n",
    "Function: Neurons in different layers contribute to feature extraction (in hidden layers) and final decision-making (in output layers). They help in processing and learning from data by mapping complex relationships between inputs and outputs.\n",
    "\n",
    "### 2.Connections:\n",
    "\n",
    "Role: Connections between neurons represent the pathways that transmit signals from one neuron to another. Each connection carries a signal from the output of one neuron to the input of another in the following layer.\n",
    "Function: These connections allow the network to form a complex structure where information is passed and transformed through layers, enabling the network to learn hierarchical representations.\n",
    "\n",
    "### 3.Weights:\n",
    "\n",
    "Role: Weights are parameters associated with connections between neurons that determine the strength of the signal being transmitted. They control how much influence one neuron has on another.\n",
    "Function: During the training process, the network adjusts the weights to optimize the learning process. Weights are updated using optimization algorithms like gradient descent to minimize the loss function, allowing the network to learn and make more accurate predictions.\n",
    "\n",
    "### 4.Biases:\n",
    "\n",
    "Role: Biases are additional parameters added to the weighted sum before the activation function is applied. They allow the activation function to shift left or right, which helps the network better fit the data.\n",
    "Function: Biases provide flexibility in the learning process by enabling neurons to output non-zero values even when all input values are zero. This helps the network learn complex patterns and make accurate predictions by shifting the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a353ccb-9c26-4f83-860d-de28c08d6f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cce95c01-2f51-4858-99c1-c21f5cc7b6b4",
   "metadata": {},
   "source": [
    "## 4.Illustrate the architecture of an artificial neural network. Provide an example to explain the flow of information through the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039454c9-21f6-4d23-831a-4554a1ed9b6b",
   "metadata": {},
   "source": [
    "### Architecture of an Artificial Neural Network (ANN):\n",
    "An artificial neural network typically consists of three main types of layers:\n",
    "\n",
    "1.Input Layer: Receives the input data. Each neuron in this layer represents a feature of the input data.\n",
    "\n",
    "2.Hidden Layers: Intermediate layers where computations are performed to learn patterns from the data. These layers apply weights, biases, and activation functions to process the information.\n",
    "\n",
    "3.Output Layer: Produces the final result or prediction. The number of neurons in this layer depends on the specific task (e.g., one neuron for binary classification, multiple neurons for multi-class classification).\n",
    "                                                                                                                           \n",
    "#### Illustrative Example:\n",
    "Imagine a simple neural network for binary classification (e.g., predicting whether an email is spam or not) with the following structure:\n",
    "\n",
    "Input Layer: 3 neurons, each representing a feature such as \"Number of links\", \"Use of certain keywords\", and \"Length of the email\".\n",
    "\n",
    "Hidden Layer: 2 neurons, each applying a non-linear activation function to the weighted input from the input layer.\n",
    "\n",
    "Output Layer: 1 neuron, outputting a value between 0 and 1 after applying a sigmoid activation function to indicate the probability of the email being spam.\n",
    "\n",
    "### Flow of Information:\n",
    "\n",
    "1.Input Layer:\n",
    "\n",
    "The input features (e.g., the number of links, keywords, length) are fed into the input neurons.\n",
    "\n",
    "2.Hidden Layer:\n",
    "\n",
    "Each input value is multiplied by the respective weight associated with the connection to each neuron in the hidden layer.\n",
    "A weighted sum is computed, and a bias is added.\n",
    "The result is passed through an activation function (e.g., ReLU or sigmoid) to introduce non-linearity.\n",
    "\n",
    "3.Output Layer:\n",
    "\n",
    "The outputs from the hidden layer are multiplied by their respective weights and passed through an activation function (e.g., sigmoid for binary classification).\n",
    "The final output represents the network’s prediction (e.g., a value close to 1 indicating spam, and close to 0 indicating not spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38079297-16e9-4b6e-a6dc-9f45832bbe30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44e2f172-c994-4aec-9a75-e2e62bc5d185",
   "metadata": {},
   "source": [
    "## 5.Outline the perceptron learning algorithm. Describe how weights are adjusted during the learning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02a07d-84e5-48ea-884f-40b59a2b17a6",
   "metadata": {},
   "source": [
    "### Perceptron Learning Algorithm Overview:\n",
    "\n",
    "The perceptron is one of the simplest types of artificial neural networks and is used for binary classification. It consists of a single neuron that receives input features, applies weights to them, sums them up, and passes the result through an activation function to produce an output. The perceptron learning algorithm is used to train this single-layer neural network.\n",
    "\n",
    "Weight Adjustment Explanation:\n",
    "\n",
    "Learning rate (η): This parameter controls how much the weights are adjusted in response to the error. A small learning rate leads to slow learning, while a large learning rate may cause the model to converge too quickly to a suboptimal solution or oscillate around the optimal solution.\n",
    "\n",
    "Weight update: The weights are adjusted in the direction that reduces the error. If 𝑦>𝑦 (i.e., the actual output is 1 but the predicted output is 0), the weights are increased to increase the likelihood of predicting 1 in the future. Conversely, \n",
    "    if 𝑦<𝑦 , the weights are decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c92825-cd66-472c-a932-a201e35c57c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7148e54e-e3d9-43c6-b119-fd0475baba0f",
   "metadata": {},
   "source": [
    "## 6.Discuss the importance of activation functions in the hidden layers of a multi-layer perceptron. Provide examples of commonly used activation functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671f0b9-7f3a-4d5f-a313-107509f08c45",
   "metadata": {},
   "source": [
    "### Importance of Activation Functions in Hidden Layers of a Multi-Layer Perceptron (MLP):\n",
    "\n",
    "Activation functions are vital components of the hidden layers in a multi-layer perceptron (MLP) because they introduce non-linearity into the network. This non-linearity allows the network to learn complex patterns and relationships in the input data. Without activation functions, regardless of the number of layers, the network would only be able to model linear functions because the composition of linear functions is still linear. Non-linear activation functions enable MLPs to approximate complex, non-linear mappings between inputs and outputs, making them powerful tools for tasks such as image recognition, speech processing, and complex decision-making.\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "### Examples of Common Activation Functions:\n",
    "\n",
    "#### 1.ReLU (Rectified Linear Unit):\n",
    "\n",
    "Function: 𝑓(𝑥)=max⁡(0,𝑥) \n",
    "\n",
    "Advantages: Fast computation, reduces vanishing gradient problem.\n",
    "\n",
    "Disadvantages: Can cause \"dying ReLU\" where some neurons never activate.    \n",
    "\n",
    "#### 2.Sigmoid:\n",
    "\n",
    "Function: 𝑓(𝑥)=1/1+e −x \n",
    " \n",
    "Advantages: Smooth and outputs values between 0 and 1, good for probability modeling.\n",
    "\n",
    "Disadvantages: Prone to vanishing gradient for large input magnitudes.\n",
    "\n",
    "#### 3.Tanh (Hyperbolic Tangent):\n",
    "\n",
    "Function: f(x)=tanh(x)\n",
    "\n",
    "Advantages: Zero-centered, helps with convergence.\n",
    "\n",
    "Disadvantages: Still suffers from vanishing gradient for extreme values.\n",
    "\n",
    "#### 4.Leaky ReLU:\n",
    "\n",
    "Function: \n",
    "𝑓(𝑥)= x if x>0,else 𝑓(𝑥)=𝛼𝑥 (e.g., α=0.01)\n",
    "\n",
    "Advantages: Prevents \"dying ReLU\" by allowing a small gradient when x<0.\n",
    "\n",
    "Disadvantages: Choosing α can be tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d1490-9400-4ef7-ba9c-d026f8fbf803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c64f8a32-64ba-446e-9c67-7d12e2bec2b5",
   "metadata": {},
   "source": [
    "# Various Neural Network Architect Overview Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6befe2-2944-4169-8fe0-2c3dbbaffc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af79b07c-7f10-4318-b466-ee3d4585d68e",
   "metadata": {},
   "source": [
    "## 1. Describe the basic structure of a Feedforward Neural Network (FNN). What is the purpose of the activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15724ae1-2049-4fe2-a677-724d13d55534",
   "metadata": {},
   "source": [
    "### Basic Structure of a Feedforward Neural Network (FNN):\n",
    "A Feedforward Neural Network (FNN) is a type of artificial neural network where the data moves in only one direction: forward from the input layer through the hidden layers to the output layer. There are no cycles or loops in this structure, which is why it is called \"feedforward.\"\n",
    "\n",
    "#### Components of an FNN:\n",
    "\n",
    "1.Input Layer: This layer receives the input features and passes them to the next layer.\n",
    "\n",
    "2.Hidden Layers: One or more layers where the input is processed through weighted connections and passed through an activation function. These layers allow the network to learn complex representations.\n",
    "\n",
    "3.Output Layer: Produces the final output of the network, which could be a prediction or classification, depending on the task (e.g., single value for regression, probabilities for classification).\n",
    " Each layer is made up of neurons (nodes) that perform calculations using weights, biases, and an activation function.\n",
    "\n",
    "### Purpose of the Activation Function:\n",
    "The activation function introduces non-linearity into the network. This is crucial because it allows the network to model complex relationships between inputs and outputs. Without an activation function, the network would only perform linear transformations, which limits its ability to solve complex tasks.\n",
    "\n",
    "By applying an activation function to the weighted sum of inputs at each neuron, the network can learn non-linear patterns and interactions, enabling it to approximate complex functions, make predictions, and solve a wide range of tasks such as image recognition, natural language processing, and more.                                                                                                                               \n",
    "                                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba38ead-0053-4dd3-887c-5364ead3afd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9265949-8fb1-454f-85e1-b9628661cba7",
   "metadata": {},
   "source": [
    "## 2 Explain the role of convolutional layers in CNN. Why are pooling layers commonly used, and what do they achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d07981d-78b8-4f5f-b0ee-58bedcf1a9d3",
   "metadata": {},
   "source": [
    "### Role of Convolutional Layers in CNN: \n",
    "Convolutional layers in a Convolutional Neural Network (CNN) are responsible for extracting features from the input data, such as images. They apply a set of filters (kernels) to the input to create feature maps that highlight important patterns, like edges, textures, and shapes. This process allows the network to detect hierarchical patterns, from simple edges in early layers to complex objects in deeper layers. Convolutional layers help reduce the number of parameters and computations, making the network more efficient.\n",
    "\n",
    "### Why Pooling Layers are Commonly Used:\n",
    "Pooling layers are used in CNNs to down-sample the feature maps, which reduces the spatial dimensions while retaining the most important information. This helps to decrease the computational load, reduce overfitting, and make the network more robust to variations like translation and distortion in the input.\n",
    "\n",
    "### What Pooling Layers Achieve:\n",
    "\n",
    "1.Dimensionality Reduction: Decreases the number of computations needed, speeding up training and inference.\n",
    "\n",
    "2.Feature Invariance: Helps the network become more invariant to small changes in the input, such as shifts and distortions.\n",
    "\n",
    "3.Prevention of Overfitting: By reducing the feature map size, pooling layers contribute to simplifying the model and reducing the risk of overfitting.\n",
    "\n",
    "Example: Max pooling is the most common pooling method, where the maximum value in a local region of the feature map is taken, capturing the most prominent features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b6cec-3e47-4cba-8bf9-8e296c997ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59f4ef43-49ac-4dc9-9838-389629c847e1",
   "metadata": {},
   "source": [
    "## \n",
    "3 What is the key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks? How does an RNN handle sequential data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1641e24-1653-4747-a5cc-f15af2e904ff",
   "metadata": {},
   "source": [
    "### Key Characteristic Differentiating RNNs:\n",
    "The key characteristic that differentiates Recurrent Neural Networks (RNNs) from other types of neural networks is their ability to maintain a memory of previous inputs through feedback connections. This allows RNNs to process sequential data and retain information about past inputs, making them suitable for tasks where the order and context of the data are important, such as language modeling, time-series prediction, and speech recognition.\n",
    "\n",
    "### How RNNs Handle Sequential Data:\n",
    "RNNs handle sequential data by maintaining a hidden state (memory) that gets updated at each time step. Here's how they work:\n",
    "\n",
    "1.Input Processing: At each time step t, the RNN receives an input xt and combines it with the previous hidden state h(t−1)\n",
    "\n",
    "2.Hidden State Update: The input xt and the previous hidden state ℎ𝑡−1 are passed through a neural network layer, typically with a non-linear activation function, to compute the current hidden state ht\n",
    "\n",
    "3.Output Generation: The hidden state ℎ𝑡 can be used to produce an output 𝑦𝑡 for that time step, or it can be passed to the next time step as context for processing future inputs.\n",
    "\n",
    "This process allows RNNs to retain context and make predictions based on the sequence of data, not just individual data points. However, traditional RNNs have limitations with long-term dependencies due to vanishing gradient problems, which are addressed by more advanced versions like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91681134-31ca-4059-90b6-b6d2970a473c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e25f5698-787a-49f9-93e0-6dbcb0553258",
   "metadata": {},
   "source": [
    "## 4 .Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the vanishing gradient problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26533448-d051-4633-8ac8-ff202e9f86fa",
   "metadata": {},
   "source": [
    "### Components of a Long Short-Term Memory (LSTM) Network:\n",
    "LSTM networks are a type of Recurrent Neural Network (RNN) designed to better handle long-term dependencies in sequential data. They consist of the following key components:\n",
    "\n",
    "1.Cell State (𝐶𝑡): The cell state is the memory of the LSTM that runs through the entire sequence, acting like a conveyor belt that carries relevant information from one time step to the next.\n",
    "\n",
    "2.Forget Gate (ft): This gate decides what information from the cell state should be discarded. It takes the previous hidden state ℎ𝑡−1 and the current input xt, and outputs a value between 0 and 1 for each number in the cell state, indicating how much of each component to forget.\n",
    "\n",
    "3.Input Gate (it): The input gate determines what new information should be added to the cell state. It includes a sigmoid layer that decides which values to update and a tanh layer to create new candidate values that could be added to the cell state.\n",
    "\n",
    "4.Cell State Update: The cell state is updated by combining the old cell state 𝐶𝑡−1 with the new candidate values, scaled by the input gate's output. The forget gate controls how much of the old cell state is kept, while the input gate controls the amount of new information added.\n",
    "\n",
    "5.Output Gate (ot): This gate determines what part of the cell state should be output as the hidden state ℎ𝑡. It uses a sigmoid function to decide which parts of the cell state to output and a tanh function to scale the output to be between -1 and 1.\n",
    "\n",
    "### How LSTM Addresses the Vanishing Gradient Problem:\n",
    "The vanishing gradient problem in traditional RNNs occurs because the gradients of the loss function can become extremely small as they are propagated backward through many time steps. This makes it difficult for the network to learn long-term dependencies since the updates to weights become negligible.\n",
    "\n",
    "LSTM networks address this problem through their unique architecture:\n",
    "\n",
    "The cell state acts as a long-term memory that is less affected by vanishing gradients, as it is updated in a way that allows information to flow across many time steps with minimal alteration.\n",
    "\n",
    "The forget gate and input gate control what information is retained or discarded, ensuring that relevant data can persist across time steps without vanishing.\n",
    "\n",
    "The output gate allows the network to selectively expose parts of the cell state to the next layer, enabling it to propagate meaningful information.\n",
    "\n",
    "By maintaining a stable cell state and controlling the flow of information with gates, LSTM networks can learn long-term dependencies without the vanishing gradient issue that affects traditional RNNs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c1844-af3d-4aef-be2a-d2d7d922c4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "798a0399-3bc5-4077-beaf-b1d97dea8c39",
   "metadata": {},
   "source": [
    "## 5 Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is the training objective for each?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757b4e4-6229-4750-b083-0e402079bcc7",
   "metadata": {},
   "source": [
    "### Roles of the Generator and Discriminator in a Generative Adversarial Network (GAN):\n",
    "\n",
    "A Generative Adversarial Network (GAN) consists of two neural networks, the generator and the discriminator, that are trained simultaneously through an adversarial process.\n",
    "\n",
    "#### Generator: Creates fake data (e.g., images) to mimic real data and tries to fool the discriminator.\n",
    "Discriminator: Distinguishes between real data and fake data created by the generator.\n",
    "Training Objectives:\n",
    "\n",
    "Generator: Aims to minimize the discriminator's ability to tell real from fake data, making the generated data as realistic as possible.\n",
    "Discriminator: Aims to maximize its accuracy in correctly classifying real and fake data.\n",
    "The two networks compete in an adversarial game, improving each other until the generator produces highly realistic data that the discriminator can no longer distinguish from real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4bc18f-a0b8-4083-95a7-f0eecdb4e579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b8ad1-0011-46b1-8bf1-7f19ea292806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
