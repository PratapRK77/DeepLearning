{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd680021-a350-40ac-95ce-f82c15a0ecff",
   "metadata": {},
   "source": [
    "## Forward and Backward Propagation Assignment questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a3b88-adb7-4340-b916-dfbb5c4af7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7be439a9-3bf5-452a-b8df-016839f57568",
   "metadata": {},
   "source": [
    "### 1.Explain the concept of forward propagation in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979662d-1b1d-4050-929f-5a32b07925e8",
   "metadata": {},
   "source": [
    "Forward propagation is the process of passing input data through a neural network to produce an output. It is a key step in training and using neural networks for tasks like classification, regression, or any predictive modeling. The concept involves a series of computations through the network's layers, where each layer transforms its input into an output using a set of weights, biases, and an activation function.\n",
    "\n",
    "Here's a detailed breakdown of forward propagation:\n",
    "\n",
    "#### 1. Input Layer\n",
    "The input layer receives the raw data (e.g., images, text, or numerical values).\n",
    "Each input feature is assigned to a neuron in this layer.\n",
    "\n",
    "#### 2. Hidden Layers\n",
    "Each neuron in a hidden layer computes a weighted sum of its inputs. Mathematically:\n",
    "z (l) = W(l) ⋅a(l−1) + b(l)\n",
    "where:\n",
    "\n",
    "𝑧(𝑙) : Weighted sum for the l-th layer.\n",
    "W (l) : Weight matrix connecting layer l−1 to layer 𝑙\n",
    "a (l−1) : Activations (output) from the previous layer.\n",
    "b (l) : Bias vector for the L-th layer.\n",
    "The result 𝑧(𝑙)  is then passed through an activation function f to introduce non-linearity:\n",
    "a(l) =f(z(l)) # Z POWER L\n",
    "Common activation functions include ReLU, sigmoid, and tanh.\n",
    "This process is repeated for all neurons in the hidden layers.\n",
    "\n",
    "#### 3. Output Layer\n",
    "The final layer of the network aggregates the results from the last hidden layer and produces the network's output. For example:\n",
    "Regression tasks: The output is a single number (e.g., using no activation or linear activation).\n",
    "Classification tasks: The output is a probability distribution (e.g., using a softmax activation).\n",
    "\n",
    "#### 4. Example of Forward Propagation\n",
    "Consider a simple network with:\n",
    "\n",
    "1 input layer with two features (𝑥1,𝑥2)\n",
    "\n",
    "1 hidden layer with two neurons, and\n",
    "\n",
    "1 output neuron.\n",
    "\n",
    "Step-by-Step:\n",
    "1.Compute the weighted sum and activation for the first hidden layer:\n",
    "𝑧1=𝑤11𝑥1+𝑤12𝑥2+𝑏1,𝑎1 = 𝑓(𝑧1)\n",
    "\n",
    "𝑧2=𝑤21𝑥1+𝑤22𝑥2+𝑏2,𝑎2=𝑓(𝑧2)\n",
    "\n",
    "2.Compute the output:\n",
    "𝑧out =𝑤𝑜1𝑎1+𝑤𝑜2𝑎2+𝑏out,𝑦=𝑓(𝑧out)\n",
    "\n",
    "\n",
    "#### 5. Purpose of Forward Propagation\n",
    "To generate predictions: This is used during inference.\n",
    "To calculate the loss: During training, forward propagation is followed by backpropagation, where the network updates its weights to minimize the loss.\n",
    "Forward propagation is efficient and is the forward \"pass\" in the neural network's operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2b282-e922-40aa-9d30-0e78770131d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aef8abe-4020-4a8d-b537-d9f506716574",
   "metadata": {},
   "source": [
    "## Q2.What is the purpose of the activation function in forward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2984f-27c2-46d8-b277-e5f0cc73fb6f",
   "metadata": {},
   "source": [
    "The purpose of the activation function in forward propagation is to enhance the functionality and expressiveness of a neural network by introducing non-linearity and controlling the output of neurons. Here are the key purposes in detail:\n",
    "\n",
    "#### 1. Introducing Non-Linearity\n",
    "Real-world data often involves complex, non-linear relationships. Activation functions allow the network to learn and model such patterns.\n",
    "Without non-linearity, the network would be limited to solving only linearly separable problems, regardless of its depth.\n",
    "Activation functions transform the linear combinations of inputs and weights into non-linear outputs, enabling the network to approximate any function.\n",
    "\n",
    "#### 2. Allowing Hierarchical Feature Learning\n",
    "In multi-layer neural networks, activation functions enable each layer to learn more abstract and meaningful features from the previous layer's output.\n",
    "Example: In an image classifier, early layers might learn edges, while deeper layers learn complex shapes or objects.\n",
    "This progressive abstraction is crucial for tasks like image recognition, language processing, and other complex problems.\n",
    "\n",
    "#### 3. Controlling the Range of Outputs\n",
    "Activation functions often restrict the output to a specific range (e.g., 0 to 1, −1 to 1).\n",
    "This helps:\n",
    "Prevent large, unbounded values from destabilizing the network.\n",
    "Provide interpretable outputs, such as probabilities in classification tasks (e.g., sigmoid or softmax).\n",
    "\n",
    "#### 4. Enabling Backpropagation\n",
    "Most activation functions are differentiable, which is essential for backpropagation during training.\n",
    "Backpropagation relies on the derivative of the activation function to compute gradients for adjusting weights and biases.\n",
    "Choosing an activation function with an appropriate gradient helps ensure effective learning.\n",
    "\n",
    "#### 5. Improving Model Performance\n",
    "Different activation functions are suited to different tasks, and choosing the right one can significantly affect the network's performance:\n",
    "Avoiding vanishing gradients: ReLU (Rectified Linear Unit) and its variants help address the vanishing gradient problem that occurs with sigmoid or tanh in deep networks.\n",
    "Sparsity: ReLU introduces sparsity by outputting zero for negative inputs, which can improve computational efficiency and reduce overfitting.\n",
    "\n",
    "In summary, the activation function transforms the raw outputs of neurons in a way that allows the neural network to learn non-linear patterns, represent hierarchical features, stabilize computations, and support the training process via backpropagation. It is a critical component that makes deep learning practical and effective for complex problems.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78769e7-6d62-4096-81c8-c12cca6f961d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "851dbb9f-c773-4d6f-b662-547c4683ca4d",
   "metadata": {},
   "source": [
    "## 3.Describe the steps involved in the backward propagation (backpropagation) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4034483-800c-443b-9051-42e3b6c3d46d",
   "metadata": {},
   "source": [
    "Here is a concise summary of the steps involved in the backpropagation algorithm:\n",
    "\n",
    "#### 1. Forward Pass\n",
    "Pass input data through the network to compute the predicted output.\n",
    "\n",
    "Calculate the loss (error) using a loss function.\n",
    "\n",
    "#### 2. Compute Gradients at the Output Layer\n",
    "Calculate the gradient of the loss with respect to the output layer’s pre-activation values (z(L)) using the chain rule.\n",
    "\n",
    "Compute gradients of the weights and biases in the output layer.\n",
    "\n",
    "#### 3. Backward Pass Through Hidden Layers\n",
    "\n",
    "For each hidden layer:\n",
    "Calculate the error term (𝛿(𝑙) ) using the weights and errors from the next layer.\n",
    "\n",
    "Compute gradients of the weights and biases in the current layer.\n",
    "\n",
    "\n",
    "#### 4. Update Weights and Biases\n",
    "    \n",
    "Adjust the weights and biases using an optimization algorithm like gradient descent:\n",
    "    W (l) ←W (l) −η ⋅ ∂L/ ∂W (l)\n",
    "\n",
    "    b (l) ←b (l) −η⋅ ∂L / ∂b(l)\n",
    " \n",
    "#### 5. Repeat\n",
    "Iterate steps 1–4 for multiple epochs or until the loss converges.\n",
    "\n",
    "In essence, backpropagation computes gradients using the chain rule, propagates the error backward through the network, and updates parameters to minimize the loss.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d9b21-6d49-42f4-9e12-272924c3a22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a91fc1cc-21bb-411f-9245-c265b97910a8",
   "metadata": {},
   "source": [
    "## 4.What is the purpose of the chain rule in backpropagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10842b-4025-477c-a9dd-b1da9026d98c",
   "metadata": {},
   "source": [
    "The chain rule is fundamental to the backpropagation algorithm as it enables the calculation of gradients for deep neural networks. Specifically, it allows the error (loss) to be propagated backward from the output layer to the earlier layers, ensuring that each layer's weights and biases are updated correctly. Here's the purpose of the chain rule in backpropagation:\n",
    "\n",
    "#### 1. Efficient Gradient Computation\n",
    "The chain rule provides a systematic way to compute the gradient of the loss function with respect to each weight and bias in the network, even when the network has many layers.\n",
    "It breaks the computation into manageable steps by considering the relationships between successive layers.\n",
    "\n",
    "#### 2. Linking Layers in the Network\n",
    "In a neural network, the output of one layer is the input to the next. The chain rule helps in computing how the change in a weight or bias in one layer affects the loss, accounting for all intermediate transformations.\n",
    "\n",
    "Mathematically : ∂L/∂W(l) =∂L/∂z(l) ⋅ ∂z(l)/∂W(l)\n",
    "\n",
    "#### 3. Handling Non-Linear Activation Functions\n",
    "Neural networks use non-linear activation functions, making direct gradient computation challenging. The chain rule enables differentiation through these non-linearities by combining their derivatives with those of the previous layers.\n",
    "\n",
    "#### 4. Backward Propagation of Error\n",
    "The chain rule allows errors to flow backward through the network:\n",
    "\n",
    "The gradient of the loss at the output layer is computed first.\n",
    "\n",
    "This gradient is then propagated backward to compute gradients for all preceding layers by chaining the partial derivatives layer by layer. \n",
    "\n",
    "#### 5. Parameter Optimization\n",
    "The gradients computed using the chain rule are used in optimization algorithms (e.g., gradient descent) to update weights and biases, minimizing the loss function.\n",
    "\n",
    "##### The purpose of the chain rule in backpropagation is to compute the gradient of the loss with respect to each weight and bias in a multi-layer neural network. It achieves this by breaking the gradient computation into smaller steps, linking the layers, and propagating the error backward from the output to the input. This allows efficient and accurate updates of the model parameters during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde4772-5c39-4a64-a7fd-92ac5309d427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b1b3c47-6312-451b-9292-a6042cf28667",
   "metadata": {},
   "source": [
    "## 5.Implement the forward propagation process for a simple neural network with one hidden layer using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc365c27-e5c2-4530-8dc0-b31aef5bd58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer activations (A1):\n",
      "[[0.35205505 1.05616565 0.        ]\n",
      " [0.         0.         0.48509093]\n",
      " [0.         0.         0.        ]\n",
      " [0.99475361 1.42281433 0.        ]]\n",
      "\n",
      "Output layer activations (A2):\n",
      "[[0.16227489 0.10235562 0.32089971]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the activation function (ReLU for hidden layer, sigmoid for output layer)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the forward propagation function\n",
    "def forward_propagation(X, weights, biases):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through a neural network with one hidden layer.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data, shape (n_features, n_samples)\n",
    "    - weights: A dictionary with weights for the hidden and output layers\n",
    "    - biases: A dictionary with biases for the hidden and output layers\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing intermediate and final outputs (activations)\n",
    "    \"\"\"\n",
    "    # Compute the hidden layer\n",
    "    Z1 = np.dot(weights['W1'], X) + biases['b1']  # Weighted sum for hidden layer\n",
    "    A1 = relu(Z1)                                # Activation for hidden layer\n",
    "\n",
    "    # Compute the output layer\n",
    "    Z2 = np.dot(weights['W2'], A1) + biases['b2']  # Weighted sum for output layer\n",
    "    A2 = sigmoid(Z2)                               # Activation for output layer\n",
    "\n",
    "    # Store intermediate results for potential backpropagation\n",
    "    activations = {\n",
    "        'Z1': Z1, 'A1': A1,\n",
    "        'Z2': Z2, 'A2': A2\n",
    "    }\n",
    "    return activations\n",
    "\n",
    "# Example setup\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Input data (2 features, 3 samples)\n",
    "X = np.array([[0.5, 1.5, -1.0],\n",
    "              [1.0, -0.5,  2.0]])\n",
    "\n",
    "# Neural network parameters\n",
    "weights = {\n",
    "    'W1': np.random.randn(4, 2),  # 4 neurons in hidden layer, 2 input features\n",
    "    'W2': np.random.randn(1, 4)  # 1 output neuron, 4 hidden neurons\n",
    "}\n",
    "biases = {\n",
    "    'b1': np.random.randn(4, 1),  # Bias for 4 hidden neurons\n",
    "    'b2': np.random.randn(1, 1)  # Bias for 1 output neuron\n",
    "}\n",
    "\n",
    "# Perform forward propagation\n",
    "activations = forward_propagation(X, weights, biases)\n",
    "\n",
    "# Output results\n",
    "print(\"Hidden layer activations (A1):\")\n",
    "print(activations['A1'])\n",
    "print(\"\\nOutput layer activations (A2):\")\n",
    "print(activations['A2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f77d53-cbb7-4735-8650-6d75459c1c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e88a9-c41e-4c83-9b72-44fd0f048287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
